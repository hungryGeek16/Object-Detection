{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"SocialDistancing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"clip.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145656920"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "print(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1c89decbbe91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfourcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"codec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*args[\"codec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--videopath VIDEOPATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/rahul/.local/share/jupyter/runtime/kernel-f34a0abd-4bbc-41be-a82c-526c92716cba.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.read()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_H = cap.read()[1].shape[0]\n",
    "IMAGE_W = cap.read()[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = np.float32([[0, IMAGE_H], [IMAGE_W, IMAGE_H], [0, 0], [IMAGE_W, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0., 1080.],\n",
       "       [1920., 1080.],\n",
       "       [   0.,    0.],\n",
       "       [1920.,    0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<VideoCapture 0x7f92912fadd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"clip.mp4\")\n",
    "while True:\n",
    "    __, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (640, 480), interpolation = cv2.INTER_LINEAR)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    H,W = frame.shape[0:2]\n",
    "    for output in layerOutputs:\n",
    "    # loop over each of the detections\n",
    "        for detection in output:\n",
    "        # extract the class ID and confidence (i.e., probability) of\n",
    "        # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "        # filter out weak predictions by ensuring the detected\n",
    "        # probability is greater than the minimum probability\n",
    "            if confidence > 0.5:\n",
    "            # scale the bounding box coordinates back relative to the\n",
    "            # size of the image, keeping in mind that YOLO actually\n",
    "            # returns the center (x, y)-coordinates of the bounding\n",
    "            # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "            # use the center (x, y)-coordinates to derive the top and\n",
    "            # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "            # update our list of bounding box coordinates, confidences,\n",
    "            # and class IDs\n",
    "                if classID == 0:\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "                idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5,0.3)\n",
    "                if len(idxs) > 0:\n",
    "                    for i in idxs.flatten():\n",
    "                        (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                        (w, h) = (boxes[i][2], boxes[i][3])\n",
    "    # draw a bounding box rectangle and label on the image\n",
    "                        cv2.rectangle(frame, (x, y), (x + w, y + h), (102,220,225), 2)\n",
    "                        text = \"{}: {:.4f}\".format(classIDs[i], confidences[i])\n",
    "                        cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (102,220,225), 2)\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "        break#When everything's done, release capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mouse_points(event, x, y, flags, param):\n",
    "    # Used to mark 4 points on the frame zero of the video that will be warped\n",
    "    # Used to mark 2 points on the frame zero of the video that are 6 feet away\n",
    "    global mouseX, mouseY, mouse_pts\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        mouseX, mouseY = x, y\n",
    "        cv2.circle(image, (x, y), 10, (0, 255, 255), 10)\n",
    "        if \"mouse_pts\" not in globals():\n",
    "            mouse_pts = []\n",
    "        mouse_pts.append((x, y))\n",
    "        print(\"Point detected\")\n",
    "        print(mouse_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"clip.mp4\")\n",
    "cv2.namedWindow(\"image\")\n",
    "cv2.setMouseCallback(\"image\", get_mouse_points)\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"clip.mp4\")\n",
    "while True:\n",
    "    __, frame = cap.read()\n",
    "    cv2.circle(frame, (150, 120), 10, (0, 255, 255), 10)\n",
    "    cv2.circle(frame, (1500, 120), 10, (0, 255, 255), 10)\n",
    "    cv2.circle(frame, (150, 700), 10, (0, 255, 255), 10)\n",
    "    cv2.circle(frame, (1500, 700), 10, (0, 255, 255), 10)\n",
    "    \n",
    "    pts = np.float32([[150,120],[1500,120],[150,700],[1500,700]])\n",
    "    pts2 = np.float32([[0,0],[1920,0],[0,1080],[1920,1080]])\n",
    "    \n",
    "    matrix = cv2.getPerspectiveTransform(pts,pts2)\n",
    "    \n",
    "    result = cv2.warpPerspective(frame,matrix,(1920,1080))\n",
    "\n",
    "                       \n",
    "    cv2.imshow(\"result\",result)\n",
    "    if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-3536577eb94e>, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-3536577eb94e>\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    if len(idxs) > 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "mouse_pts = []\n",
    "\n",
    "\n",
    "def get_mouse_points(event, x, y, flags, param):\n",
    "    # Used to mark 4 points on the frame zero of the video that will be warped\n",
    "    # Used to mark 2 points on the frame zero of the video that are 6 feet away\n",
    "    global mouseX, mouseY, mouse_pts\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        mouseX, mouseY = x, y\n",
    "        cv2.circle(image, (x, y), 10, (0, 255, 255), 10)\n",
    "        if \"mouse_pts\" not in globals():\n",
    "            mouse_pts = []\n",
    "        mouse_pts.append((x, y))\n",
    "\n",
    "\n",
    "input_video = \"video.mp4\"\n",
    "\n",
    "# Define a DNN model\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "scale_w = 1.2 / 2\n",
    "scale_h = 4 / 2\n",
    "# Setup video writer\n",
    "# Initialize necessary variables\n",
    "frame_num = 0\n",
    "total_pedestrians_detected = 0\n",
    "total_six_feet_violations = 0\n",
    "total_pairs = 0\n",
    "abs_six_feet_violations = 0\n",
    "pedestrian_per_sec = 0\n",
    "sh_index = 1\n",
    "sc_index = 1\n",
    "\n",
    "cv2.namedWindow(\"image\")\n",
    "cv2.setMouseCallback(\"image\", get_mouse_points)\n",
    "num_mouse_points = 0\n",
    "first_frame_display = True\n",
    "\n",
    "# Process each frame, until end of video\n",
    "while cap.isOpened():\n",
    "    frame_num += 1\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"end of the video file...\")\n",
    "        break\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    H,W = frame.shape[0:2]\n",
    "    for output in layerOutputs:\n",
    "    # loop over each of the detections\n",
    "        for detection in output:\n",
    "        # extract the class ID and confidence (i.e., probability) of\n",
    "        # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "        # filter out weak predictions by ensuring the detected\n",
    "        # probability is greater than the minimum probability\n",
    "            if confidence > 0.5:\n",
    "            # scale the bounding box coordinates back relative to the\n",
    "            # size of the image, keeping in mind that YOLO actually\n",
    "            # returns the center (x, y)-coordinates of the bounding\n",
    "            # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "            # use the center (x, y)-coordinates to derive the top and\n",
    "            # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "            # update our list of bounding box coordinates, confidences,\n",
    "            # and class IDs\n",
    "                if classID == 0:\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "                idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5,0.3)\n",
    "                if len(idxs) > 0:\n",
    "                    for i in idxs.flatten():\n",
    "                        (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                        (w, h) = (boxes[i][2], boxes[i][3])\n",
    "    # draw a bounding box rectangle and label on the image\n",
    "                        cv2.rectangle(frame, (x, y), (x + w, y + h), (102,220,225), 2)\n",
    "                        text = \"{}: {:.4f}\".format(classIDs[i], confidences[i])\n",
    "                        cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (102,220,225), 2)\n",
    "                            \n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 'yolov3-tiny.weights'\n",
    "configPath = 'yolov3-tiny.cfg'\n",
    "import cv2\n",
    "import numpy as np\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weights)\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "H,W = None,None\n",
    "input_video = \"video.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(input_video)  # Path to the video input \n",
    "(W, H) = (None, None) # Initialize width and height variables\n",
    "q = 0  # to store intital value of width and reduce the width of the image \n",
    "frame_count = 0\n",
    "while True:\n",
    "\n",
    "    (grabbed, frame) = cap.read() #reads image frame by frame\n",
    "    frame_count += 1\n",
    "    if not grabbed: # If no frame left then loop breaks.(grabbed is a bool value which tells whether frame is present or not)\n",
    "        break\n",
    "\n",
    "    if W is None or H is None: #The whole purpose of loop is to initialize the height and width of the vidoe frame\n",
    "        (H, W) = frame.shape[:2] # for once\n",
    "        q = W\n",
    "\n",
    "    frame = frame[0:H, 200:q] #Reduced some amount of frame width\n",
    "    (H, W) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "                                 swapRB=True, crop=False) # Initialization of blob of size 416,416 from the input frame\n",
    "    net.setInput(blob)  # Passing on the input to yolo architecture\n",
    "    layerOutputs = net.forward(ln) # Outputs the array containing bounding boxes,confidences and class Ids.\n",
    "    # yolo is big architecture so it outputs values at checkpoints named yolo_16 and yolo_23.\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    for output in layerOutputs:  # For each output checkpoint\n",
    "\n",
    "        for detection in output: # For each detection in those checkpoints\n",
    "\n",
    "            scores = detection[5:] # Scores are appended at every output of layers in yolo architecture.\n",
    "            # We consider the best or the max value among them\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID] # Score of the detected person\n",
    "            if classID == 0: # Here yolo is basically object detection but we need only persons for our problems\n",
    "            # Hence filter all those class labels which are not humans.    \n",
    "                if confidence > 0.5: # Confidence thresholding\n",
    "                    box = detection[0:4] * np.array([W, H, W, H]) # Detections are scaled values, hence bringing them back to original sizes\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                    x = int(centerX - (width / 2)) # Here we determine the starting point of bounding box\n",
    "                    y = int(centerY - (height / 2))\n",
    "\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3) # Non Maximum Supression for redudant bounding boxes.\n",
    "\n",
    "    if len(idxs) > 0:\n",
    "        contact = list()   \n",
    "        persons = idxs.flatten() # Flatten all those detected value indices\n",
    "        persons_in_risk =list() # Initialize list for persons at risk\n",
    "        low_risk = list() #Initialize list for persons at risk\n",
    "        centers = list() # Initialize centers list for every detected bounding box\n",
    "        distance = list()\n",
    "        for i in persons:\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            centers.append([int(x + w / 2), int(y + h / 2)]) # calculate center of bounding box for every person detected.\n",
    "\n",
    "            contact.append(0) # initialize contact list with zeros\n",
    "        for i in range(len(centers)):  # Now calculate  distances from every bounding box center and classifiy them\n",
    "            # into three categories, high risk,low risk and no risk.\n",
    "            for j in range(i+1,len(centers)):\n",
    "                flag = isclose(centers[i], centers[j])\n",
    "                if flag == 1:\n",
    "                    persons_in_risk.append([centers[i], centers[j]])\n",
    "                    contact[i] = 1\n",
    "                    contact[j] = 1\n",
    "                elif flag == 2:\n",
    "                    low_risk.append([centers[i], centers[j]])\n",
    "                    contact[i] = 2\n",
    "                    contact[j] = 2\n",
    "\n",
    "        all_persons = len(centers) # Keep count of all category persons\n",
    "        people_at_low_risk = contact.count(2)\n",
    "        people_at_high_risk = contact.count(1)\n",
    "        people_at_no_risk = contact.count(0)\n",
    "        kk = 0\n",
    "\n",
    "        for i in persons:\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            if contact[kk] == 1:\n",
    "                # Construction of rectange in every category\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 150), 4)\n",
    "            elif contact[kk] == 0:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "\n",
    "            else:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 120, 255), 4)\n",
    "\n",
    "            kk += 1\n",
    "            # Show risk lines between high and low risk categories\n",
    "        for h in persons_in_risk:\n",
    "            cv2.line(frame, tuple(h[0]),tuple(h[1]), (0, 0, 255), 2)\n",
    "        for b in low_risk:\n",
    "            cv2.line(frame, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)\n",
    "            \n",
    "        # Plot count of every category person on the detection screen and monitor social distancing.    \n",
    "        tot_str = \"Persons detected: \" + str(all_persons)\n",
    "        high_str = \"People at high risk:(Red_Boxes):\" + str(people_at_high_risk)\n",
    "        low_str = \"People at low risk(Orange_boxes):\" + str(people_at_low_risk)\n",
    "        safe_str = \"People at no risk(Green_boxes): \" + str(people_at_no_risk)        \n",
    "        \n",
    "        cv2.putText(frame,tot_str , (80,50), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(frame,high_str , (80,80), font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(frame,low_str , (80,110), font, fontScale, (0,165,255), thickness, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(frame,safe_str , (80,150), font, fontScale, (0,255,0),thickness, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        cv2.imshow('Social Distancing', frame)\n",
    "        if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrated_dist(p1, p2):\n",
    "    return ((p1[0] - p2[0]) ** 2 + 550 / ((p1[1] + p2[1]) / 2) * (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "    # Calculation of distance between pair of bounding boxes.\n",
    "\n",
    "def isclose(p1, p2):\n",
    "    c_d = calibrated_dist(p1, p2)\n",
    "    calib = (p1[1] + p2[1]) / 2\n",
    "    if 0 < c_d < 0.15 * calib: # For high risk personality\n",
    "        return 1\n",
    "    elif 0 < c_d < 0.2 * calib: # For low risk personality\n",
    "        return 2\n",
    "    else:                      # For no risk personality\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "  \n",
    "# org \n",
    "org = (80, 80) \n",
    "  \n",
    "# fontScale \n",
    "fontScale = 1\n",
    "   \n",
    "# Blue color in BGR \n",
    "color = (0,0,0) \n",
    "  \n",
    "# Line thickness of 2 px \n",
    "thickness = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolo_16', 'yolo_23']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  
